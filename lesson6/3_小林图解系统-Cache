
1. 三级缓存结构
    L1: 每一个核心都独有一份 L1 
            ==> 每一个 L1 有细分两份
            ==> 1. 指令缓存
            ==> 2. 数据缓存
    L2: 每一个核心独有一份 L2 
    L3: 所有核心共享一份 L3
    如图: https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84/CPU-Cache.png


2. 三级缓存: 
    上通: cpu寄存器
    下达: 内存
3. CPU-Cache 是如何读取数据,并加载的
    读取粒度: Cache 如果想从内存加载某一个数据, 一定糊加载一个内存块
                ==> 暨: 并不单单只是这一个数据,
                ==> 因为 一次性加载,连续的一小段连续的内存空间,可以有效地减少,Cache 和内存交互的次数
                ------------> 粒度取决于:  coherency_line_size 参数 一般是64字节
                暨Cache块大小: 64字节


    Cache:内部数据组织形式: 组相连映射
            csdn博客参考:https://blog.csdn.net/weixin_43274923/article/details/107435447
            一针见血图解:https://img-blog.csdnimg.cn/20200718222751312.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzI3NDkyMw==,size_16,color_FFFFFF,t_70


            组相连映射描述:
                    Cache 的内部 将总共空间 分成 Q各组
                    主存 将所有空间 分成若干个区, 每个区之中 的Cache 块数 == Cache 组数
                    映射时   主存中任何一个区中的第0块可以放到cache中第0组的任意一块，
                            主存中任何一个区中的第1块可以放到cache中第1组的任意一块，
                            主存中任何一个区中的第n块可以放到cache中第n组的任意一块。


            CPU使用虚拟地址访问Cache    
                    虚拟地址 = 组标记 + 索引 + 偏移量 
                            ==> 组标记: cache中的哪一个组
                            ==> 索引 : 每一个组内可能有很多个cache块,具体是哪一个内存块
                            ==> 偏移量: 这个64字节内部 第多少个字节
                            图片: https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E7%BC%93%E5%AD%98/%E7%9B%B4%E6%8E%A5Cache%E6%98%A0%E5%B0%84.png


===============================================
为什么要使用Cache
    cpu和内存之间有着巨大的速度鸿沟
为什么要是用多级缓存
    单层缓存,在没有命中时,依然会去和 主存进行交互,
    多级缓存在当前层级没有命中时,会去和下一级缓存交互 ==> 多级缓存,进一步见笑了 缓存未命中时 和主存交互的性能损失
为什么L1缓存要区分成指令缓存and 数据缓存
    cpu内部的寄存器: 分为指令寄存器,和数据寄存器
        缓存功能细分,可以进一步保证程序局部性原则, 从而进一步提高性能

缓存命中: 要查找的数据直接就在Cache里面,不需要和内存进行交互
缓存未命中: 暨缓存缺失, 需要进一步向下级缓存查找,或者和内存进行交互
    在内存中命中后, 先直接交给CPU,然后尝试写入缓存
        如果缓存位置足够:   写入缓存
        如果缓存位置不够: 采取相关策略算法


===================================================
如何写出CPU跑的更快的代码? 暨如何提高缓存命中率
    提升 指令缓存 命中率:    有规律的条件分支语句能够让 CPU 的分支预测器发挥作用，进一步提高执行的效率；
            1. 分支预测器:  
                    ```
                        对于 if 条件语句，意味着此时至少可以选择跳转到两段不同的指令执行，
                        也就是 if 还是 else 中的指令。
                        那么，如果分支预测可以预测到接下来要执行 if 里的指令，
                        还是 else 指令的话，就可以「提前」把这些指令放在指令缓存中，
                        这样 CPU 可以直接从 Cache 读取到指令，于是执行速度就会很快。
                    ```
    提升 数据缓存 命中率:
        按照内存布局顺序访问，
            将可以有效的利用 CPU Cache 带来的好处
            这样我们代码的性能就会得到很大的提升，




==========================================
cache 缓存一致性
1. 单核cpu
    读取cache 不会触发一致性问题
    写入cache,同步内存: 可能触发一致性问题
            1. 写直达: 每一次写入Cache时,同时写入内存
                        ==> 效率比较低
            2. 写回:  每一次写入cache时,检查是否为脏 , 为脏则先刷脏,在写入
                        不为脏,则直接写入,并标记为藏
                        ==> 大大减小了 cache和内存的交互
2. 多核心CPU如何保证缓存一致性
        大思想: 
                1. 写传播: 当某个 CPU 核心发生写入操作时，需要把该事件广播通知给其他核心；
                2. 事务的串行化: 是事物的串行化，这个很重要，只有保证了这个，才能保障我们的数据是真正一致的，我们的程序在各个不同的核心上运行的结果也是一致的；

        实现方式:
                1. 写传播: 总线嗅探技术
                2. 事务的串行化: MESI协议 (有限状态机)
                    ```
                    MESI 协议其实是 4 个状态单词的开头字母缩写，分别是：
                        Modified，已修改
                        Exclusive，独占
                        Shared，共享
                        Invalidated，已失效
                    ```
                    补充一下MESI协议，MESI分别代表了高速缓存行的四种状态：
最开始只有一个核读取了A数据，此时状态为E独占，数据是干净的，
后来另一个核又读取了A数据，此时状态为S共享，数据还是干净的，
接着其中一个核修改了数据A，此时会向其他核广播数据已被修改，让其他核的数据状态变为I失效
而本核的数据还没回写内存，状态则变为M已修改，
等待后续刷新缓存后，数据变回E独占，其他核由于数据已失效，
读数据A时需要重新从内存读到高速缓存，此时数据又共享了
